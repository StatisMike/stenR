---
title: "Procedural and Object-oriented workflows of stenR"
output: rmarkdown::html_vignette
fig_width: 8
fig_height: 4 
vignette: >
  %\VignetteIndexEntry{Procedural and Object-oriented workflows of stenR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 6,
  fig.height = 4,
  fig.align = "center"
)
```

```{r setup, include = FALSE}
library(stenR)
```

`stenR` provides tools that can be used in fully procedural way, best known
to most `R` users, as well as object-oriented way.

Both of these ways can be beneficial and useful on their own: 

- procedural:
   - could be more handy for useRs not accustomed with `R6` classes
   - catered for one-offs: when you have gathered a lot of raw scores and there
   is a need to calculate score tables, standardize them
- object-oriented:
   - easily normalize multiple discrete numeric variables
   - catered for inter-session continuity: automatically append new standardized
   values to already computed score tables

## Procedural workflow for using stenR functionality

`stenR` currently is based on few `S3` classes and can be used in purely
procedural workflow, which will be described below. Exemplary data provided
with package will be used:

```{r exemplary_data}
library(stenR)

# data gathered in Polish sample - 204 summed results of HEXACO-60 questionnaires
str(HEXACO_60)
```

### 1. Create a Frequency Table object on basis of some variable

```{r FrequencyTable_create}
HEX_H_ft <- FrequencyTable(HEXACO_60$HEX_H)
HEX_H_ft
```

Generated warning tells us that there were some raw scores between `min()` and
`max()` values that weren't represented in the sample. We can see that was the 
case for a few scores:

```{r missing_scores}
which(HEX_H_ft$table$n == 0)
```

We can also see it while plotting the resulting *FrequencyTable*:

```{r FrequencyTable_plot}
plot(HEX_H_ft)
```

By the rule of thumb: the more possible values the raw scores can get and the 
smaller your data, the bigger possibility for this to happen. If you feel that 
your sample was representative, you can ignore it. It is recommended though to 
get more varied, greater number of observations when getting this warning.

### 2. Generate ScoreTable using FrequencyTable

*FrequencyTable* is a basis for normalizing the distribution of your data. Now,
the next step is to standardize it using some *StandardScale* and generating
*ScoreTable*.

There are some *StandardScale* objects available in the package. They define
score scale. We can use the popular in psychology **STEN** scale to calculate
the scores.

```{r STEN_scale}
# check out the scale definition
STEN
# see its distribution graphically
plot(STEN)
```

Now, we can generate the ScoreTable using our FrequencyTable and scale of our
choosing.

```{r ScoreTable}
HEX_H_st <- ScoreTable(HEX_H_ft, STEN)

plot(HEX_H_st)
```

We can see that the shape of the generated distribution is similar to the
distribution associated with the *StandardScale*. It is a good sign: number
of values of raw scores is appropriate for the chosen scale. In contrast, we
can see that the **TANINE** scale would be an ill choice:

```{r HEXACO_tanine}
plot(ScoreTable(HEX_H_ft, TANINE))
```

You can also define your own *StandardScale* object using the `StandardScale`
function.

```{r StandardScale}
new_scale <- StandardScale("my_scale", 10, 3, 0, 20)

# let's see if everything is correct
new_scale

# how does its distribution looks like?
plot(new_scale)
```


### 3. Calculate score for the new results

Now, if we have our *ScoreTable* created, we can use it to standardize some
scores! We will do it for some randomly generated ones using `normalize_score()`
function:

```{r}
set.seed(2137)

# generate some random raw scores of valid values
raw_scores <- round(runif(10, min = 10, max = 50), 0)

print(raw_scores)

# and now - get the 'STEN' values!
normalize_score(
  x = raw_scores,
  table = HEX_H_st,
  what = "sten")
```

## Object oriented workflow

In addition to procedural workflow described above, there is also an `R6` class
definition prepared to handle the creation of *ScoreTables* and generation
of normalized scores: *CompScoreTable*.

There is one useful feature of this object, mainly the ability to automatically
recalculate *ScoreTables* based on raw score values calculated using the `standardize`
method. It can be helpful for inter-session continuity.

### Initialize the object

During object initialization you can attach some previously calculated *FrequencyTables*
and/or *StandardScales*. It is fully optional, as it can also be done afterwards.

```{r init_CompScoreTable}
# attach during initialization
HexCST <- CompScoreTable$new(
  tables = list(HEX_H = HEX_H_ft),
  scales = STEN
)

# attach later
altCST <- CompScoreTable$new()
altCST$attach_FrequencyTable(HEX_H_ft, "HEX_H")
altCST$attach_StandardScale(STEN)

# there are no visible differences in objects structure
summary(HexCST)
summary(altCST)
```

### Expand *CompScoreTable*

After creation the object can be expanded with more *FrequencyTables* and *StandardScales.*
All *ScoreTables* will be internally recalculated

```{r expand_CST}
# add new FrequencyTable
HexCST$attach_FrequencyTable(FrequencyTable(HEXACO_60$HEX_C), "HEX_C")
summary(HexCST)

# add new StandardScale
HexCST$attach_StandardScale(STANINE)
summary(HexCST)
```

### Standardize scores

After the object is ready, the score standardization may begin. Let's feed
it some raw scores!

```{r CST_standardize}
# standardize the Honesty-Humility and Consciousness
HexCST$standardize(
  data = head(HEXACO_60),
  what = "sten",
  vars = c("HEX_H", "HEX_C")
)

# you can also do this easily with pipes!
HEXACO_60[1:5, c("HEX_H", "HEX_C")] |>
  # no need to specify 'vars', as the correct columns are already selected
  HexCST$standardize("sten")
```

### Automatically recalculate ScoreTables

During score standardization, you can also automatically add new raw scores
to existing frequencies and recalculate the *ScoreTables* automatically.

It is done before returning the values, so they will be based on the most recent
ScoreTables.

```{r CST_append}
# check the current state of the object
summary(HexCST)

# now, standardize and recalculate!
HEXACO_60[1:5, c("HEX_H", "HEX_C")] |>
  HexCST$standardize("sten", calc = TRUE)

# check the new state
summary(HexCST)
```

### Export tables

There is also option to export the *ScoreTables* - either to use them later 
in procedural way or to create new *CompScoreTable* in another session - 
for this reason there is also option to export them as *FrequencyTables*!

```{r CST_export}
# export as ScoreTables
st_list <- HexCST$export_ScoreTable()
summary(st_list)

# export as FrequencyTables
ft_list <- HexCST$export_ScoreTable(strip = T)
summary(ft_list)
```

## Calculate FrequencyTable using only distribution data

Ideally, the normalization using `stenR` should be done only using raw, actual
data. Unfortunately, it often can't be accessed.

Alternatively, the articles specifying measurement construction often share the 
descriptive statistics of their own results. Using them we can create *Simulated*
tables:

```{r SimTables}
sim_ft <- SimFrequencyTable(min = 10, max = 50, M = 31.04, 
                            SD = 6.7, skew = -0.3, kurt = 2.89, seed = 2678)

class(sim_ft)

plot(sim_ft)
```

The *Simulated* class will be inherited by *ScoreTable* object created on its basis.

Simulated tables can be used in every way that regular ones can be with one
exception: if used to create *CompScoreTable* object, the raw scores cannot be
appended to this kind of table in `standardize()` method.

```{r check_SimComp, error=TRUE}
SimCST <- CompScoreTable$new(
  tables = list("simmed" = sim_ft),
  scales = STEN
)

SimCST$standardize(
  data = data.frame(simmed = round(runif(10, 10, 50), 0)),
  what = "sten",
  calc = TRUE)

```