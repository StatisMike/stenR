---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  fig.width = 6,
  fig.height = 4
)
Sys.setenv(LANGUAGE='en')
```

# stenR <img src='man/figures/logo.png' align="right" width="200" style="float:right; width:200px !important;"/>

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-blue.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
[![codecov](https://codecov.io/gh/StatisMike/stenR/branch/master/graph/badge.svg?token=H62VR1J454)](https://codecov.io/gh/StatisMike/stenR)
<!-- badges: end -->

`stenR` is a package tailored mainly for users and creators of psychological questionnaires,
though other social science researchers and survey authors can benefit greatly
from it.

It provides straightforward framework for normalization and standardization
of raw discrete scores to standardized scale of your choosing. It follows simple
work pattern:

- create frequency table and compute Z-score corresponding to each raw score
- create score table using some standard scale.
- provide external raw score to be recalculated to standardized scale

Additionally, it introduces compatible with rest of the workflow helper
functions to preprocess data: from user answers on each item, to summarized
scales/factors score.

## Installation

You can install the current version from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("StatisMike/stenR")
```

## Usage

There are numerous functions and `S3` classes constructors to help normalize
and standardize discrete data.

```{r procedural_use}
library(stenR)

# build-in dataset with questionnaire data
summary(HEXACO_60$HEX_C)

# create STEN score table for variable
HEX_C_st <- HEXACO_60$HEX_C |>
  FrequencyTable() |>
  ScoreTable(STEN)

# use the STEN table to normalize and standardize data
HEX_C_sten <- HEXACO_60$HEX_C |>
  normalize_score(HEX_C_st, "sten")

summary(HEX_C_sten)
```

There is also a special `R6` class making the whole process more organized 
with potential inter-session continuity.

```{r oop_use}
# inspect the whole dataset for comparison
head(HEXACO_60)

# initialize the object with some scale attached
HEX_ST <- CompScoreTable$new(
  scales = STEN
)

# you can use the `standardize` method to automatically calculate the tables
# during data calculation
HEXACO_sten <- HEX_ST$standardize(
  HEXACO_60,
  # specify the variables to calculate if there are any other in the data.frame
  vars = c("HEX_H", "HEX_E", "HEX_X", "HEX_A", "HEX_C", "HEX_O"),
  what = "sten",
  calc = TRUE)

# inspect the recalculated dataset
head(HEXACO_sten)

# CompScoreTable object have the table already populated for next data input
summary(HEX_ST)
```

Above methods were focusing on using raw data available to us. Unfortunately,
when using another author's questionnaire we often don't have their raw data
available for calculating the *FrequencyTable* in regular way.

Authors usually provide descriptive statistics about their data, though.
We can use it to estimate the frequencies and use it to normalize raw scores
gathered in our study.

```{r simulated_use}
simulated_ft <- SimFrequencyTable(
  min = 10, max = 50, M = 31.04, SD = 6.7, 
  skew = -0.3, kurt = 2.89, seed = 2678)

plot(simulated_ft)

normalize_score(HEXACO_60$HEX_A,
                table = ScoreTable(simulated_ft, STEN),
                "sten") |>
  summary()
```

## Data preprocessing

Workflow above assumes that your data is already preprocessed, and summarized
score for each construct is presented to the functions.

When working with actual data, it is most commonly presented as answers
to individual items. It is then needed to summarize them into distinct
scales or factors. Author may also chose to handle `NA`s in some way, and
commonly there are items in need of reversing before summarizing into scale score.

All of these steps can be achieved using `ScaleSpec` objects with `sum_items_to_scale` function.

```{r SLCS_preprocess}
# Answers per item: Self-liking self-competence scale
# (dataset included within package)
head(SLCS)

# Scale specification for each of subscales and general score:
## Self-Liking specification
SL_spec <- ScaleSpec(
  name = "SL",
  item_names = paste("SLCS", c(1, 3, 5, 6, 7, 9, 11, 15), sep = "_"),
  reverse = paste("SLCS", c(1, 6, 7, 15), sep = "_"),
  min = 1,
  max = 5)

## Self-Competence specification
SC_spec <- ScaleSpec(
  name = "SC",
  item_names = paste("SLCS", c(2, 4, 8, 10, 12, 13, 14, 16), sep = "_"),
  reverse = paste("SLCS", c(8, 10, 13), sep = "_"),
  min = 1,
  max = 5)

## General Score specification
GS_spec <- ScaleSpec(
  name = "GS",
  item_names = paste("SLCS", 1:16, sep = "_"),
  reverse = paste("SLCS", c(1, 6, 7, 8, 10, 13, 15), sep = "_"),
  min = 1, 
  max = 5)

# Sum the data
SLCS_summed <- sum_items_to_scale(SLCS, SL_spec, SC_spec, GS_spec, retain = c("user_id", "sex", "age"))
head(SLCS_summed)
```